# Методология анализа данных и машинного обучения
**Автор:** Волошина Виктория Николаевна
**Категория:** Data Science / Методология анализа данных
**Формат:** Конспект 

## Оглавление
1. Этапы анализа данных
2. Методы анализа и обработки данных
3. Машинное обучение
4. Инструменты Python для машинного обучения
5. Регрессия
6. Классификация
7. Деревья решений
8. Ансамбли моделей
9. Управление компьютерными инцидентами (ГОСТ)
10. Нормативная база управления ИБ (УИИБ)
11. Кластеризация
12. OSINT: технологии, методики, подходы
13. Системы массового обслуживания (СМО)

---

# 1. Этапы анализа данных
# Лекция по теме: Этапы анализа данных

## Введение

Анализ данных — это упорядоченный процесс преобразования необработанной информации в знания, которые используются для принятия решений. В современных организациях данные играют ключевую роль, а качество анализа напрямую влияет на эффективность бизнес-процессов, научных исследований и автоматизированных систем.

Этапы анализа данных стандартизированы во многих методологиях: CRISP-DM, KDD, SEMMA и др. Несмотря на различия в деталях, все подходы включают сходные шаги:

* подготовку данных,
* исследование (EDA),
* построение моделей,
* оценку качества,
* внедрение.

---

# Основные этапы анализа данных

## 1. Определение цели анализа

Перед обработкой данных важно определить:

* проблему, которую необходимо решить;
* ключевые метрики качества;
* требования заказчика;
* ограничения времени, ресурсов и качества.

На этом этапе формулируются:

* бизнес-цель,
* аналитическая задача (классификация, прогнозирование, сегментация),
* критерии успеха модели.

---

## 2. Сбор данных

Источники данных могут быть:

* базы данных,
* веб-сайты,
* API,
* сенсоры и IoT-устройства,
* файлы (CSV, JSON, Excel),
* ручной ввод.

Важно фиксировать:

* происхождение данных;
* частоту обновления;
* структуру полей;
* наличие пропусков.

Хорошо документированные данные обеспечивают прозрачность анализа.

---

## 3. Очистка данных (Data Cleaning)

Это один из самых важных и трудоёмких этапов.

### Включает:

* устранение пропущенных значений,
* удаление выбросов,
* исправление некорректных форматов,
* обработку дубликатов,
* нормализацию данных.

Некачественные данные приводят к неверным выводам, поэтому правило Data Science:

> **70% успеха — качественная подготовка данных.**

---

## 4. Исследовательский анализ данных (EDA)

EDA помогает понять структуру и распределение данных, выявить закономерности.

Основные действия:

* построение графиков (гистограммы, боксплоты, корреляционные матрицы);
* анализ взаимосвязей между признаками;
* выявление групп, трендов и аномалий;
* оценка распределений.

Цель EDA — сформировать гипотезы, которые затем проверяются моделированием.

---

## 5. Преобразование данных (Feature Engineering)

На этом этапе создают новые признаки и подготавливают данные для моделей.

### Типичные операции:

* стандартизация и нормализация;
* кодирование категориальных признаков;
* генерация производных признаков;
* извлечение признаков из текста, изображений, сигналов;
* отбор наиболее значимых признаков.

Feature engineering непосредственно влияет на качество моделей.

---

## 6. Построение модели

Выбор модели зависит от задачи:

* классификация → логистическая регрессия, деревья решений, SVM, нейросети;
* регрессия → линейная регрессия, Random Forest, градиентный бустинг;
* кластеризация → k-means, DBSCAN;
* временные ряды → ARIMA, Prophet;
* рекомендации → коллаборативная фильтрация, матричная факторизация.

На этом этапе проводится настройка гиперпараметров.

---

## 7. Оценка модели

Используются метрики:

### Для классификации:

* accuracy, recall, precision, F1-score;
* ROC-кривая, AUC.

### Для регрессии:

* MSE, RMSE, MAE, R².

### Для кластеризации:

* силуэтный коэффициент;
* внутрикластерное расстояние.

Модель считается качественной, если она удовлетворяет бизнес-критериям, а не только математическим метрикам.

---

## 8. Внедрение и интерпретация результатов

На завершающем этапе выполняются:

* интеграция модели в рабочие процессы,
* мониторинг качества,
* документирование,
* интерпретация выводов.

Важно обеспечить:

* обновление модели,
* контроль деградации качества,
* корректную визуализацию результатов.

---

# Итоги

Этапы анализа данных формируют системный подход к работе с информацией. Они обеспечивают:

* структурированность процесса,
* предсказуемость результатов,
* высокое качество моделирования.

Следующие лекции будут развивать отдельные аспекты методологии анализа данных: методы обработки, машинное обучение, ансамбли моделей и нормативную базу.

---

# 2. Методы анализа и обработки данных
# Лекция по теме: Методы анализа и обработки данных

## Введение

Методы анализа данных — ключевой инструмент Data Science. Они позволяют выявлять закономерности, уточнять гипотезы и подготавливать данные к моделированию.
Обработка данных — это совокупность действий, направленных на структуирование, преобразование и подготовку данных для дальнейшего анализа или построения моделей машинного обучения.

Лекция рассматривает основные подходы:

* статистический анализ,
* математическую обработку,
* визуализацию данных,
* методы снижения размерности,
* предобработку числовых, категориальных и временных данных.

---

# Классификация методов анализа данных

Методы анализа делят на несколько категорий.

## 1. Описательная статистика (Descriptive Statistics)

Позволяет описать свойства выборки с помощью:

* среднего
* медианы
* моды
* дисперсии
* стандартного отклонения
* квартилей

Используется для первичного понимания структуры данных.

Пример:

> Среднее значение продаж за месяц → 54 000 ₽.

---

## 2. Корреляционный анализ

Корреляция показывает, как изменяется один признак относительно другого.

Типы корреляций:

* Пирсона (линейная),
* Спирмена (монотонная),
* Кендэлла (порядковая).

Корреляционная матрица помогает:

* выявить зависимые признаки,
* избежать мультиколлинеарности,
* сократить лишние признаки.

---

## 3. Методы визуализации данных

Визуализация помогает увидеть структуру данных, которую сложно распознать при просмотре таблиц.

Популярные графики:

* гистограммы,
* box-plot (ящик с усами),
* scatter-plot (точечная диаграмма),
* pairplot (попарный анализ признаков),
* heatmap (картинка-корреляция),
* lineplot (временные ряды).

Цель: увидеть аномалии, выбросы, распределение.

---

# Методы обработки данных

## 1. Работа с пропущенными значениями

Пропуски могут возникать из-за ошибок измерений, отсутствия информации, некорректного импорта.

Стратегии обработки:

* удаление строк;
* удаление признаков;
* заполнение средним/медианой;
* заполнение наиболее частым значением;
* предсказание пропусков с помощью модели.

Лучший подход выбирают по характеру данных.

---

## 2. Работа с выбросами

Выбросы — значения, сильно отклоняющиеся от нормы.

Методы выявления:

* IQR (межквартильный размах),
* Z-score,
* визуальный анализ (box-plot),
* локальные методы (LOF).

Методы обработки:

* удаление выбросов,
* исправление значений,
* преобразование логарифмом,
* разбиение данных на группы.

---

## 3. Масштабирование данных (Scaling)

Важно для методов, чувствительных к масштабу:

* kNN,
* SVM,
* линейные модели,
* нейросети.

Методы масштабирования:

### StandardScaler

(приведение к нулевому среднему и единичному отклонению)

### MinMaxScaler

(масштабирование в диапазон [0,1])

### RobustScaler

(устойчив к выбросам)

---

## 4. Кодирование категориальных признаков

Категориальные признаки необходимо преобразовать в числовые.

Популярные методы:

### One-Hot Encoding

Создаёт бинарные столбцы для каждого значения категории.

### Label Encoding

Заменяет категории числами.

### Ordinal Encoding

Для упорядоченных категорий (низкий/средний/высокий уровень).

### Target Encoding

Категории кодируются средним значением целевой переменной.

Применяют с осторожностью из-за риска переобучения.

---

## 5. Снижение размерности

Цель — уменьшить количество признаков без потери смысла.

Популярные методы:

### PCA (Principal Component Analysis)

Преобразует данные в новое пространство признаков.

### t-SNE

Хорош для визуализации структур данных.

### LDA

Используется при классификации.

Преимущества:

* ускорение обучения моделей,
* устранение мультиколлинеарности,
* снижение переобучения.

---

## 6. Обработка временных рядов

Временные данные требуют особых методов:

* скользящее среднее,
* экспоненциальное сглаживание,
* сезонное разложение,
* лаговые признаки,
* скользящие окна.

Эти методы позволяют выявлять тенденции, сезонность и циклы.

---

## 7. Работа с текстовыми данными

Методы:

* токенизация,
* стемминг,
* лемматизация,
* удаление стоп-слов,
* n-граммы,
* TF-IDF.

Используются для подготовки текста к моделям NLP.

---

# Итоги

Методы анализа и обработки данных являются фундаментом работы аналитика и специалиста по машинному обучению. Они позволяют:

* структурировать данные,
* выявлять закономерности,
* готовить данные к моделированию,
* улучшать качество моделей.

Эти знания переходят в следующую тему — **Машинное обучение**, где рассмотрены алгоритмы обучения и типы моделей.

---

# 3. Машинное обучение

# Лекция по теме: Машинное обучение

## Введение

Машинное обучение (Machine Learning, ML) — это область искусственного интеллекта, изучающая методы построения алгоритмов, которые могут *самостоятельно обучаться* на данных и *делать прогнозы* без явного программирования всех правил.

ML применяется в:

* системах рекомендаций,
* прогнозировании спроса,
* медицине,
* финансовом анализе,
* компьютерном зрении,
* обработке естественного языка,
* кибербезопасности и обнаружении аномалий.

Основная идея: модель учится на примерах и выделяет закономерности, которые затем применяет к новым данным.

---

# Подходы к машинному обучению

ML делят на несколько ключевых направлений.

---

## 1. Обучение с учителем (Supervised Learning)

Модели обучаются на размеченных данных, где есть входы (признаки) и правильные ответы (метки).

### Основные задачи:

### ✔️ Классификация

Предсказание категорий.
Пример: «спам / не спам».

Алгоритмы:

* логистическая регрессия,
* SVM,
* деревья решений,
* Random Forest,
* градиентный бустинг,
* нейронные сети.

---

### ✔️ Регрессия

Предсказание числового значения.
Пример: прогноз цены квартиры.

Алгоритмы:

* линейная регрессия,
* полиномиальная регрессия,
* Ridge, Lasso, ElasticNet,
* ансамбли моделей.

---

## 2. Обучение без учителя (Unsupervised Learning)

Используется, когда нет меток.

### Основные задачи:

### ✔️ Кластеризация

Поиск групп внутри данных.

Алгоритмы:

* k-means,
* DBSCAN,
* иерархическая кластеризация.

---

### ✔️ Поиск аномалий

Применяется в кибербезопасности, финансах, медицине.

Алгоритмы:

* Isolation Forest,
* One-Class SVM,
* автоэнкодеры.

---

### ✔️ Снижение размерности

PCA, t-SNE, UMAP.

Применяется для визуализации и ускорения обучения.

---

## 3. Обучение с подкреплением (Reinforcement Learning)

Агент взаимодействует с окружающей средой и получает вознаграждение за правильные действия.

Применение:

* игры (AlphaGo),
* робототехника,
* оптимизация процессов,
* системное управление.

---

# Ключевые этапы построения модели машинного обучения

Чтобы модель работала корректно, важно соблюдать этапы разработки.

---

## 1. Подготовка данных

На этом этапе выполняют:

* очистку данных,
* кодирование категориальных признаков,
* масштабирование,
* отбор важнейших признаков,
* устранение выбросов.

Качество данных влияет на точность модели больше, чем выбор алгоритма.

---

## 2. Разделение данных на выборки

Данные обычно делят на:

* **train** (обучение)
* **validation** (подбор гиперпараметров)
* **test** (финальная оценка качества)

Типичные пропорции: 70/15/15 или 80/10/10.

---

## 3. Обучение модели

Модель подбирает параметры, минимизируя ошибку.

Пример: линейная регрессия подбирает коэффициенты функции.

---

## 4. Подбор гиперпараметров

Гиперпараметры модели (например, глубина дерева, количество соседей) выбираются с помощью:

* Grid Search,
* Random Search,
* Bayesian Optimization.

---

## 5. Оценка модели

Для классификации:

* accuracy, F1, ROC-AUC.

Для регрессии:

* MSE, RMSE, MAE.

Для кластеризации:

* silhouette score.

Оценка должна быть **объективной**, не привязанной к конкретной выборке.

---

## 6. Интерпретация результатов

Важно понимать:

* какие признаки наиболее значимы,
* где модель ошибается,
* насколько она устойчива к шуму,
* как поведёт себя на новых данных.

Инструменты:

* SHAP,
* LIME,
* таблицы важности признаков.

---

# Переобучение и недообучение

## Переобучение (overfitting)

Модель слишком хорошо запомнила обучающие данные, но плохо работает на новых.

Признаки:

* высокая точность на train,
* низкая на test.

Способы устранения:

* регуляризация (L1, L2),
* увеличение данных,
* уменьшение сложности модели,
* кросс-валидация.

---

## Недообучение (underfitting)

Модель слишком простая и не улавливает закономерности.

Признаки:

* низкая точность на train и test.

Способы устранения:

* использование более сложной модели,
* добавление признаков,
* снижение регуляризации.

---

# Типы моделей машинного обучения

## 1. Линейные модели

Простые и интерпретируемые.

## 2. Деревья решений

Интуитивны, но склонны к переобучению.

## 3. Ансамблевые методы

Random Forest, Gradient Boosting — высокая точность и устойчивость.

## 4. Нейронные сети

Основа глубокого обучения (Deep Learning).

---

# Итоги

Машинное обучение — фундаментальный инструмент анализа данных.
Оно позволяет:

* автоматизировать принятие решений,
* выявлять сложные закономерности,
* строить интеллектуальные системы.

Следующая тема — **Инструменты Python, используемые в машинном обучении**, где рассматриваются платформы, библиотеки и экосистема языка.

---

# 4. Инструменты Python для машинного обучения

# Лекция по теме: Инструменты Python, используемые в машинном обучении

## Введение

Python — основной язык в Data Science и машинном обучении благодаря:

* простоте синтаксиса,
* широкому набору библиотек,
* активному сообществу,
* гибкости и расширяемости.

Инструменты Python позволяют решать задачи:

* статистического анализа,
* визуализации,
* обработки данных,
* построения моделей,
* глубокого обучения.

В лекции рассматриваются ключевые библиотеки и инструменты, используемые в ML-проектах.

---

# Основные библиотеки Python для ML

---

## 1. NumPy — фундамент для работы с массивами

**NumPy** — базовая библиотека для работы с многомерными массивами и матрицами.

Возможности:

* векторные операции,
* линейная алгебра,
* быстрые вычисления,
* генерация случайных чисел.

Особенность: операции выполняются значительно быстрее обычных циклов Python.

---

## 2. Pandas — обработка и анализ данных

**Pandas** предоставляет структуры данных:

* `Series` — одномерная структура;
* `DataFrame` — таблица, удобная для анализа.

Возможности:

* импорт данных из CSV, Excel, SQL;
* фильтрация и сортировка;
* обработка пропусков;
* группировка данных;
* работа с временными рядами.

Pandas — основной инструмент анализа данных.

---

## 3. Matplotlib и Seaborn — визуализация данных

### Matplotlib — базовая библиотека графиков:

* линейные графики,
* гистограммы,
* scatter plot,
* bar chart.

### Seaborn — надстройка над Matplotlib:

* более красивые и информативные графики;
* heatmap для корреляций;
* violin plot;
* pairplot.

Визуализация — ключевой этап EDA.

---

## 4. Scikit-learn — основная библиотека ML

Самый популярный инструментарий для машинного обучения.

Содержит:

* классификацию,
* регрессию,
* кластеризацию,
* деревья решений,
* ансамбли моделей,
* методы отбора признаков,
* снижение размерности.

Также включает:

* разбиение на выборки,
* масштабирование данных,
* подбор гиперпараметров.

Scikit-learn идеально подходит для классического ML.

---

## 5. SciPy — научные вычисления

Используется для:

* оптимизации,
* численных методов,
* обработки сигналов,
* статистики.

Много функций SciPy применяются для подготовки данных.

---

## 6. TensorFlow и Keras — глубокое обучение

**TensorFlow** — фреймворк от Google для работы с нейронными сетями.

Возможности:

* построение глубоких моделей,
* обучение на GPU/TPU,
* автоматическое дифференцирование,
* создание функциональных и последовательных моделей.

**Keras** — высокоуровневая надстройка над TensorFlow.

Преимущества:

* простота описания архитектур;
* удобство работы;
* высокая скорость разработки.

---

## 7. PyTorch — нейронные сети с динамическими графами

Разработан Facebook (Meta).

Преимущества:

* гибкость,
* динамическая вычислительная графика,
* удобство отладки.

Используется в научных исследованиях и продвинутых проектах DL.

---

## 8. Jupyter Notebook — интерактивная среда разработки

Jupyter Notebook позволяет:

* выполнять код по шагам,
* визуализировать результаты,
* добавлять текстовые описания,
* создавать учебные и исследовательские материалы.

Этот инструмент — стандарт де-факто в Data Science.

---

# Экосистема разработки ML-проектов

## 1. Управление зависимостями

Инструменты:

* pip
* conda
* poetry

Позволяют фиксировать версии библиотек и обеспечивать воспроизводимость.

---

## 2. Управление экспериментами

Инструменты:

* MLflow
* Weights & Biases
* Neptune.ai

Позволяют:

* отслеживать метрики,
* сохранять модели,
* вести историю экспериментов.

---

## 3. Инструменты развёртывания

Чтобы модель работала в продакшене, используют:

* FastAPI
* Flask
* Docker
* Kubernetes

Они позволяют создать API, контейнеризовать модель и масштабировать сервис.

---

# Итоги

Инструменты Python формируют целостную технологическую экосистему для машинного обучения.
Каждая библиотека выполняет свою роль:

* NumPy и SciPy — вычисления,
* Pandas — обработка данных,
* Matplotlib/Seaborn — визуализация,
* Scikit-learn — классическое ML,
* TensorFlow/PyTorch — глубокое обучение,
* Jupyter Notebook — удобная разработка.

Эти компоненты позволяют эффективно строить, анализировать и внедрять модели машинного обучения.

---

# 5. Регрессия


# Лекция по теме: Регрессия

## Введение

Регрессия — это класс моделей машинного обучения, предназначенных для предсказания **непрерывных значений**.
Примеры задач регрессии:

* прогноз стоимости квартиры;
* предсказание температуры;
* оценка спроса;
* прогноз прибыли;
* моделирование физических процессов.

Регрессионные модели часто являются базовыми в ML, так как:

* понятны,
* просты в интерпретации,
* обеспечивают хорошее качество при корректной подготовке данных.

---

# Основные типы регрессии

---

## 1. Линейная регрессия

Задача: найти линейную зависимость между **целевой переменной** и **набором признаков**.

Формула:

[
y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b
]

Где:

* (w_i) — коэффициенты модели,
* (b) — свободный член.

Преимущества:

* простота,
* интерпретируемость,
* высокая скорость обучения.

Недостатки:

* не работает, если зависимость нелинейная;
* чувствительна к выбросам;
* требует масштабирования признаков.

---

## 2. Полиномиальная регрессия

Обобщение линейной модели за счёт добавления степенных признаков.

Пример:

[
y = w_1 x + w_2 x^2 + w_3 x^3 + b
]

Преимущества:

* позволяет моделировать нелинейные зависимости.

Недостатки:

* склонность к переобучению;
* уменьшение интерпретируемости.

---

## 3. Регуляризационные модели

Используются для борьбы с переобучением.

### Ridge Regression (L2)

Наказывает большие веса:

[
\lambda \sum w_i^2
]

### Lasso Regression (L1)

Наказывает сумму модулей весов:

[
\lambda \sum |w_i|
]

Lasso может занулять коэффициенты → используется для отбора признаков.

### ElasticNet

Комбинирует L1 + L2.

---

## 4. Деревья решений и ансамбли в регрессии

Хотя деревья чаще применяются в классификации, они также хорошо работают в регрессии.

Алгоритмы:

* Decision Tree Regressor
* Random Forest Regressor
* Gradient Boosting Regressor
* XGBoost / LightGBM / CatBoost

Преимущества:

* работают с нелинейными зависимостями;
* устойчивы к выбросам;
* не требуют масштабирования.

Недостатки:

* менее интерпретируемы, чем линейные модели.

---

## 5. kNN-регрессия

Метод ближайших соседей определяет прогноз на основе средних значений соседей.

Параметры:

* k — число соседей,
* метрика расстояния (euclidean, manhattan).

Хорошо работает при локальных зависимостях.

---

# Процесс построения регрессионной модели

---

## 1. Исследование признаков

Для регрессии важны:

* распределение признаков;
* наличие выбросов;
* корреляции;
* линейные или нелинейные зависимости.

---

## 2. Масштабирование

Для линейных моделей, SVM, kNN обязательно применять scaling:

* StandardScaler
* MinMaxScaler

---

## 3. Разделение выборки

Рекомендуется:

* train/test 80/20
* или кросс-валидация (k-fold)

---

## 4. Обучение модели

Зависит от алгоритма.
Линейная регрессия решается аналитически методом:

* градиентного спуска,
* нормального уравнения.

---

## 5. Оценка качества регрессии

### Метрики:

* **MAE** — средняя абсолютная ошибка;
* **MSE** — средняя квадратичная ошибка;
* **RMSE** — корень из MSE;
* **R²** — коэффициент детерминации.

Интерпретация R²:

* 1 → идеальное объяснение данных
* 0 → модель не лучше среднего
* <0 → модель хуже константы

---

# Проблемы регрессии

---

## 1. Переобучение

Возникает при слишком сложной модели.
Решения:

* регуляризация,
* уменьшение числа признаков,
* увеличение данных.

---

## 2. Недообучение

Модель слишком простая.
Решения:

* добавление новых признаков,
* усложнение модели,
* учет нелинейности.

---

## 3. Мультиколлинеарность

Признаки сильно коррелируют между собой.

Последствия:

* нестабильность коэффициентов,
* снижение интерпретируемости.

Решения:

* PCA,
* удаление признаков,
* Ridge-регрессия.

---

# Итоги

Регрессия — базовый инструмент прогнозирования в машинном обучении.
Знание типов регрессии, метрик качества и подходов к подготовке данных позволяет эффективно решать широкий спектр задач.

Следующая тема — **Классификация**, важнейший метод предсказания категориальных значений.

---

# 6. Классификация

# Лекция по теме: Классификация

## Введение

**Классификация** — это тип задачи машинного обучения, в которой модель должна предсказать *категориальную* (дискретную) метку.
Примеры задач:

* определить, является ли письмо спамом;
* классифицировать изображение (кошка / собака / автомобиль);
* определить диагноз на основе медицинских показателей;
* предсказать, уйдёт ли клиент из компании;
* определить тональность текста.

Классификация — одна из самых распространённых задач ML.

---

# Основные типы классификации

---

## 1. Бинарная классификация

Цель — разделить объекты на два класса.

Примеры:

* Defect / No defect
* Спам / Не спам
* Уйдёт клиент / Не уйдёт

Алгоритмы:

* логистическая регрессия,
* SVM,
* деревья решений,
* случайный лес,
* градиентный бустинг,
* нейронные сети.

---

## 2. Многоклассовая классификация

Задача имеет более двух классов.

Примеры:

* классификация рукописных цифр (0–9);
* определение тематики текста;
* распознавание объектов на фото.

Алгоритмы:

* kNN,
* деревья решений,
* ансамбли,
* нейронные сети.

---

## 3. Многомаркерная (multilabel) классификация

Каждому объекту может соответствовать несколько классов одновременно.

Примеры:

* жанры фильма (драма, мелодрама, комедия);
* теги для статьи.

Подходы:

* коды бинарной релевантности,
* классификация по каждому тегу отдельно,
* нейронные сети с сигмоидой.

---

# Ключевые алгоритмы классификации

---

## 1. Логистическая регрессия

Использует сигмоиду для предсказания вероятности класса.

Функция:

[
P(y=1|x) = \sigma(w^T x + b)
]

Преимущества:

* простота и интерпретируемость;
* скорость обучения.

Ограничения:

* подходит только для линейно разделимых данных.

---

## 2. Деревья решений

Дерево рекурсивно делит пространство признаков на области.

Преимущества:

* интерпретируемость;
* работа с категориальными признаками;
* отсутствие требований к нормализации.

Недостатки:

* чувствительность к шуму;
* склонность к переобучению.

---

## 3. Случайный лес (Random Forest)

Ансамбль деревьев, обученных на разных подвыборках данных.

Преимущества:

* высокая точность,
* устойчивость к шуму,
* отсутствие переобучения при достаточном количестве деревьев.

---

## 4. Градиентный бустинг

Последовательная сборка слабых моделей (обычно деревьев).

Популярные реализации:

* XGBoost,
* LightGBM,
* CatBoost.

Преимущества:

* высокая точность,
* устойчивость к выбросам,
* работа с разными типами признаков.

---

## 5. Метод k ближайших соседей (kNN)

Простая модель:

* выбираем k ближайших объектов;
* класс определяется голосованием.

Преимущества:

* простота;
* отсутствие обучения.

Недостатки:

* большой объём вычислений на предсказании;
* чувствительность к масштабу признаков.

---

## 6. Поддерживающие векторные машины (SVM)

Ищут гиперплоскость, разделяющую классы с максимальным зазором.

Преимущества:

* высокая точность;
* работа в высоких размерностях.

---

# Метрики оценки классификации

Выбор метрик зависит от задачи.

---

## 1. Accuracy

Доля правильных предсказаний.
Хорошая метрика, когда классы сбалансированы.

---

## 2. Precision, Recall, F1-score

Используются при несбалансированных классах.

* **Precision** — точность (из всех предсказанных положительных — сколько правильных).
* **Recall** — полнота (из всех реальных положительных — сколько нашли).
* **F1-score** — гармоническое среднее.

Пример:

Если модель определяет мошенников, важнее Recall.

---

## 3. ROC-AUC

Площадь под ROC-кривой.

Показывает, насколько хорошо модель различает классы.

1.0 → идеальная модель
0.5 → случайное угадывание

---

## 4. Матрица ошибок (Confusion Matrix)

Позволяет увидеть:

* Ложноположительные ошибки (FP)
* Ложноотрицательные ошибки (FN)
* Истинные предсказания (TP, TN)

Используется в анализе качества.

---

# Переобучение в классификации

Причины:

* слишком глубокие деревья;
* слишком большое количество признаков;
* шумные данные;
* переобученные бустинги.

Решения:

* регуляризация (L1/L2);
* кросс-валидация;
* уменьшение сложности модели;
* сбор дополнительных данных.

---

# Особенности подготовки данных для классификации

---

## 1. Балансировка классов

Используют:

* undersampling,
* oversampling,
* SMOTE.

---

## 2. Масштабирование данных

Важно для:

* kNN,
* SVM,
* логистической регрессии.

---

## 3. Выбор признаков

Используют:

* mutual information,
* chi-square test,
* Lasso.

---

# Итоги

Классификация — основной тип задач ML, применяемый во всех областях анализа данных.
Грамотное применение алгоритмов классификации требует:

* понимания типов моделей;
* корректной подготовки данных;
* выбора метрик;
* анализа ошибок.

Следующая тема — **Деревья решений**, один из самых важных алгоритмов как в регрессии, так и в классификации.

---

# 7. Деревья решений


# Лекция по теме: Деревья решений

## Введение

**Деревья решений** — один из самых понятных и интуитивных алгоритмов машинного обучения.
Это модель, которая последовательно разбивает пространство признаков на области, принимая решения по простым правилам вида:

> «если признак X > порога → идти в правую ветвь, иначе — в левую».

Деревья решений используются и в **классификации**, и в **регрессии**.
Они являются базой для мощных ансамблевых методов: Random Forest, Gradient Boosting и др.

---

# Основные понятия деревьев решений

---

## 1. Узел (Node)

Точка принятия решения. Содержит правило разбиения данных.

---

## 2. Корень дерева (Root Node)

Первый узел, с которого начинается классификация/регрессия.

---

## 3. Ветви (Branches)

Пути, по которым передаются данные в зависимости от условий.

---

## 4. Листовой узел (Leaf)

Итоговое решение:

* класс (в классификации),
* числовое значение (в регрессии).

---

## 5. Глубина дерева

Максимальная длина пути от корня до листа.
Большая глубина → высокая сложность модели.

---

# Построение дерева решений

Алгоритм построения дерева является **жадным**:

> На каждом шаге выбирается локально лучшее разбиение данных.

### Этапы:

---

## 1. Выбор признака и порога разбиения

Для каждого признака перебираются возможные пороги.
Задача — найти разбиение, которое:

* максимально разделяет классы (в классификации),
* минимизирует ошибку (в регрессии).

---

## 2. Вычисление меры неоднородности

### Для классификации используют:

* **индекс Джини**
  [
  G = 1 - \sum p_i^2
  ]

* **энтропию**
  [
  H = -\sum p_i \log p_i
  ]

Обе метрики показывают, насколько «перемешаны» классы в узле.

---

### Для регрессии:

* дисперсия,
* средняя квадратичная ошибка (MSE).

---

## 3. Рекурсивное разбиение

Каждое подмножество снова делится по тому же принципу.

Остановка алгоритма наступает, когда:

* глубина превышает лимит,
* в узле мало объектов,
* дальнейшее разбиение не улучшает точность.

---

# Пример разбиения

На слайдах лекции (напр. стр. 7 и 8) демонстрируется, как дерево решений делит пространство признаков на прямоугольные области.
Границы разбиений всегда **ортогональны осям координат**, формируя последовательные прямоугольники.

---

# Деревья решений в задачах классификации

В классификации дерево:

* в каждом узле выбирает признак, который лучше всего делит классы,
* определяет порог разбиения,
* формирует листья с предсказанным классом.

Преимущества:

* высокая интерпретируемость — дерево можно визуализировать;
* простота реализации;
* работа с категориальными и числовыми признаками.

Недостатки:

* легко переобучаются;
* неустойчивость к небольшим изменениям в данных;
* сложность оптимального подбора глубины.

---

# Деревья решений в задачах регрессии

В регрессии дерево делит пространство так, чтобы:

[
\text{минимизировать } MSE = \frac{1}{n}\sum (y_i - \hat{y})^2
]

В листе хранится среднее значение целевой переменной.

Преимущества:

* моделируют нелинейные зависимости;
* устойчивы к выбросам.

Недостатки:

* результат — ступенчатая функция;
* чувствительность к глубине.

---

# Преимущества деревьев решений

* лёгкая интерпретируемость;
* не требуют нормализации данных;
* могут работать с пропусками;
* устойчивость к шумам;
* быстрый предикт.

---

# Недостатки деревьев решений

* склонность к переобучению;
* нестабильность структуры;
* небольшая точность по сравнению с ансамблями;
* разделение происходит только по одному признаку за раз.

---

# Методы уменьшения переобучения

---

## 1. Ограничение глубины дерева (max_depth)

Чем меньше глубина, тем модель проще.

---

## 2. Ограничение числа объектов в листе (min_samples_leaf)

Позволяет устранять случайные разбиения.

---

## 3. Ограничение минимального размера узла (min_samples_split)

Требует достаточного количества данных для ветвления.

---

## 4. Обрезка дерева (pruning)

Удаление узлов, не улучшающих качество.

---

# Применение деревьев решений

Используются в:

* кредитном скоринге;
* медицине;
* промышленной диагностике;
* рекомендательных системах;
* кибербезопасности;
* анализе рисков;
* распознавании образов.

---

# Итоги

Деревья решений — важная часть машинного обучения:

* просты в понимании,
* применяются в классификации и регрессии,
* образуют основу ансамблевых методов,
* требуют контроля переобучения.

Следующая тема — **Ансамбли моделей**, где деревья решений играют ключевую роль.

---

# 8. Ансамбли моделей
[...]

# 9. Управление компьютерными инцидентами (ГОСТ)


# Лекция по теме: Управление компьютерными инцидентами (ГОСТ)

## Введение

Компьютерные инциденты — это события, нарушающие политику безопасности информационной системы или представляющие угрозу её функционированию.
Управление инцидентами — один из ключевых процессов обеспечения киберустойчивости организации.

ГОСТы регулируют:

* структуру процессов реагирования,
* порядок классификации инцидентов,
* требования к действиям персонала,
* обязанности организации,
* меры по предотвращению повторных инцидентов.

Лекция основана на материалах **ГОСТ Р 56939-2016** и связанных нормативных документов.

---

# Цели управления инцидентами

Основные цели:

* обеспечение быстрого реагирования на угрозы;
* минимизация ущерба;
* восстановление работоспособности систем;
* предотвращение повторных атак;
* документирование действий;
* анализ последствий.

Процессы управления инцидентами должны быть чётко регламентированы.

---

# Ключевые понятия

---

## 1. Компьютерный инцидент

Событие, которое:

* нарушает политику безопасности,
* приводит к утечке или порче данных,
* влияет на доступность сервисов,
* задействует вредоносное ПО.

---

## 2. Информационная безопасность

Состояние защищённости:

* конфиденциальности,
* целостности,
* доступности информации.

---

## 3. Реагирование на инциденты

Комплекс организационных и технических мер, направленных на устранение последствий.

---

## 4. Участники процесса

ГОСТ выделяет:

* **Команда реагирования (CERT / SOC)**
* **Администраторы ИС**
* **Пользователи**
* **Ответственные лица руководства**
* **Внешние организации** (при необходимости)

---

# Жизненный цикл компьютерного инцидента (ГОСТ)

ГОСТ описывает полный цикл управления инцидентами:

---

## 1. Подготовка

Включает:

* разработку политики реагирования;
* определение ролей и ответственности;
* подготовку инструкций;
* обеспечение средств мониторинга;
* формирование команды реагирования.

---

## 2. Выявление (идентификация)

На этапе обнаружения используются:

* SIEM-системы;
* журналы событий;
* IDS/IPS;
* антивирусы;
* мониторинг целостности.

Цель — обнаружить аномалии и зафиксировать факт инцидента.

---

## 3. Регистрация

Инцидент документируется:

* дата и время;
* источник;
* описание событий;
* важность;
* текущее состояние.

Регистрация обязательна для анализа и отчётности.

---

## 4. Классификация

Инциденты классифицируются по:

### Важности:

* критический,
* высокий,
* средний,
* низкий.

### Типам угроз:

* вирусные атаки;
* сетевые вторжения;
* утечки данных;
* отказ сервисов;
* несанкционированный доступ.

---

## 5. Анализ и оценка

Цель:

* установить причину инцидента,
* определить масштабы ущерба,
* установить задействованные системы,
* выявить скомпрометированные учётные записи.

Применяются:

* лог-анализ,
* трассировка сетевой активности,
* форензика.

---

## 6. Реагирование

Включает меры:

* локализация инцидента (изоляция систем);
* предотвращение расширения ущерба;
* устранение последствий атаки;
* закрытие уязвимостей.

Примеры мер:

* блокировка учетных записей,
* ограничение доступа,
* остановка сетевых служб,
* удаление вредоносных файлов.

---

## 7. Восстановление

Операции после устранения угрозы:

* восстановление данных из резервных копий,
* изменение паролей,
* пересоздание учётных записей,
* восстановление доступности сервисов.

---

## 8. Постинцидентный анализ

Цель — извлечь уроки:

* что стало причиной инцидента;
* насколько эффективны меры реагирования;
* какие процессы требуют улучшения;
* какие уязвимости надо закрыть.

Создаются:

* рекомендации;
* отчёты;
* план улучшения ИБ.

---

# Требования ГОСТ к организации процесса

---

## 1. Наличие команды реагирования

Должны быть определены:

* роли,
* зона ответственности,
* каналы связи,
* график работы.

---

## 2. Наличие инструкций и регламентов

ГОСТ предписывает разработку:

* процедур уведомления,
* инструкций по сбору доказательств,
* планов реагирования.

---

## 3. Средства мониторинга

Организация должна использовать:

* средства обнаружения атак,
* защитные механизмы,
* журналы событий,
* резервирование данных.

---

## 4. Обучение персонала

Проводятся:

* тренировки,
* моделирование инцидентов,
* тестирование осведомлённости.

---

## 5. Взаимодействие с внешними структурами

При масштабных инцидентах организация обязана:

* уведомлять регуляторов (в зависимости от отрасли),
* взаимодействовать с компетентными органами,
* сообщать в государственные центры мониторинга.

---

# Типичные ошибки при реагировании

* игнорирование слабых сигналов;
* медленная реакция;
* отсутствие логов или невозможность их анализа;
* попытка скрыть инцидент;
* недостаточная документация;
* отсутствие плана восстановления.

---

# Итоги

Управление инцидентами — важный элемент киберустойчивости.
Эффективное реагирование включает:

* своевременное обнаружение,
* правильную классификацию,
* устранение угроз,
* восстановление работоспособности,
* анализ причин.

ГОСТ помогает стандартизировать процессы и обеспечить высокий уровень защиты.

---


# 10. Нормативная база управления ИБ

# Лекция по теме: Нормативная база управления информационной безопасностью (УИИБ)

## Введение

Нормативная база управления информационной безопасностью представляет собой комплекс законодательных актов, государственных стандартов, методик и внутренних регламентов, обеспечивающих защиту информации на всех этапах её жизненного цикла.

Надёжная нормативная база:

* устанавливает правила обработки и защиты информации,
* задаёт стандарты безопасности,
* определяет требования к организациям и персоналу,
* регламентирует порядок управления рисками и инцидентами.

Лекция рассматривает основные положения российской нормативной базы в области информационной безопасности (ИБ), включая законы, ГОСТы, руководящие документы и международные стандарты.

---

# Основные элементы нормативной базы УИИБ

---

## 1. Федеральные законы

Ключевые нормативные документы, определяющие правовые основы защиты информации.

### Основные ФЗ:

* **ФЗ-152 «О персональных данных»**
  Определяет требования к обработке, хранению, использованию, защите данных физических лиц.

* **ФЗ-149 «Об информации, информационных технологиях и защите информации»**
  Базовый закон для определения режима доступа к информации и мер безопасности.

* **ФЗ-187 «О безопасности критической информационной инфраструктуры» (КИИ)**
  Требует категорирования объектов и создания систем защиты.

* **ФЗ-149 и ФЗ-95 в части обеспечения безопасности государственных информационных систем**.

Законы определяют обязательства организаций и ответственность за нарушение требований ИБ.

---

## 2. Подзаконные акты и приказы регуляторов

Ключевые ведомства:

### ФСТЭК России

Регулирует:

* защита информации в государственных ИС,
* сертификация средств защиты,
* требования к СЗИ,
* аттестация ИС.

### ФСБ России

Регулирует:

* криптографические средства защиты,
* требования к защите государственной тайны,
* сертификацию средств криптографии.

### Банк России

Определяет требования в области ИБ для финансовых организаций.

---

## 3. Государственные стандарты (ГОСТы)

ГОСТы описывают требования к управлению безопасностью, процессам и техническим мерам.

### Основные ГОСТы в сфере ИБ:

* **ГОСТ Р ИСО/МЭК 27001** — система менеджмента информационной безопасности (СУИБ).
* **ГОСТ Р ИСО/МЭК 27002** — практика управления безопасностью.
* **ГОСТ Р 56939-2016** — управление инцидентами ИБ.
* **ГОСТ Р 53647.1–53647.4** — риск-менеджмент.

Стандарты помогают формализовать процессы и обеспечить соответствие лучшим мировым практикам.

---

## 4. Руководящие документы (РД)

РД ФСТЭК России и ФСБ России определяют:

* требования к защите данных;
* уровни защищённости ИС;
* порядок проведения аттестации;
* методики определения нарушителя;
* расчёт угроз.

Ключевые документы:

* **РД ФСТЭК 17** — модель угроз безопасности персональных данных.
* **РД ФСТЭК 21** — защита конфиденциальной информации.
* **РД по аттестации ИС (ФСТЭК)**.

---

## 5. Приказы и методики по защите персональных данных

Включают:

* требования по безопасности ПДн;
* уровни защищённости;
* организационные меры;
* технические меры;
* порядок ведения журналов учета.

Пример: **Приказ ФСТЭК № 21** о мерах защиты ПДн.

---

# Система менеджмента информационной безопасности (СУИБ)

СУИБ — это комплекс организационных и технических процессов, построенный на основе международного стандарта ISO 27001.

---

## Цели СУИБ

* защита информации,
* управление рисками,
* повышение устойчивости ИС,
* непрерывное улучшение процессов.

---

## Компоненты СУИБ

1. **Политика безопасности**
2. **Роли и ответственность**
3. **Оценка рисков**
4. **Меры и процедуры защиты**
5. **Мониторинг и контроль**
6. **Управление инцидентами**
7. **Обучение персонала**
8. **Аудит и корректирующие действия**

---

# Управление рисками ИБ

ГОСТ 31070 и ISO 31000 определяют методы:

* идентификации активов;
* классификации угроз;
* оценки уязвимостей;
* расчёта вероятности;
* анализа ущерба.

Методы оценки:

* качественная оценка (матрицы рисков),
* количественная (математические модели),
* комбинированная.

Результаты анализа рисков определяют выбор мер защиты.

---

# Политика информационной безопасности

Политика — основной документ ИБ, регулирующий:

* уровни доступа,
* обязанности сотрудников,
* требования к паролям,
* порядок работы с ПДн,
* правила использования сетевых ресурсов,
* ответственность за нарушение правил.

Политика обязательно утверждается руководством.

---

# Управление инцидентами (нормативный контекст)

Нормативы требуют:

* обязательной регистрации инцидентов;
* документирования всех действий;
* уведомления регуляторов (в случае инцидентов КИИ или ПДн);
* проведения расследования;
* устранения причин;
* предотвращения повторов.

ГОСТ 56939-2016 задаёт структуру полного цикла.

---

# Аудит информационной безопасности

Проводится:

* внутренними аудиторами,
* внешними организациями,
* регуляторами.

Цели:

* проверка соответствия нормам,
* выявление нарушений,
* анализ эффективности мер защиты.

---

# Обучение и осведомлённость персонала

Требования:

* обучение сотрудников ИБ;
* оценка знаний;
* регулярные тренировки;
* моделирование инцидентов.

Без подготовки персонала технические меры не обеспечивают безопасность.

---

# Типичные нарушения нормативных требований

* отсутствие документации;
* нарушение требований ФЗ-152;
* неприменение сертифицированных СЗИ;
* недокументированные действия сотрудников;
* хранение ПДн без шифрования;
* отсутствие категорирования КИИ;
* неполная регистрация инцидентов.

---

# Итоги

Нормативная база УИИБ — это система законов, приказов, ГОСТов и методик, регулирующих:

* защиту информации,
* управление рисками,
* реагирование на инциденты,
* организацию процессов ИБ,
* ответственность организаций.

Понимание нормативных требований обеспечивает:

* соответствие законодательству,
* снижение рисков,
* повышение киберустойчивости организаций.

---
# 11. Кластеризация


# Лекция по теме: Кластеризация

## Введение

Кластеризация — это метод обучения без учителя, направленный на группировку объектов так, чтобы **внутри кластера объекты были похожи друг на друга**, а **между кластерами — различались**.

Применяется в:

* маркетинге (сегментация клиентов),
* биоинформатике,
* анализе изображений,
* выявлении аномалий,
* поисковых системах,
* рекомендационных сервисах.

Алгоритмы кластеризации широко используются для предварительного анализа данных, подготовки признаков и сокращения размерности.

---

## Основные виды кластеризации

### 1. **Разделяющая (partition-based)**

Классический представитель — **k-means**.

Особенности:

* делит данные на фиксированное число кластеров;
* минимизирует внутрикластерную дисперсию;
* быстро работает, но плохо с нелинейными границами.

---

### 2. **Иерархическая кластеризация**

Создаёт дерево (дендрограмму):

* агломеративная (объединяет снизу-вверх),
* дивизивная (разделяет сверху-вниз).

Плюсы:

* не требует заранее задавать число кластеров,
* хороша для визуального анализа.

---

### 3. **Плотностная кластеризация**

Например, **DBSCAN**:

* объединяет точки, образующие плотные области,
* выделяет выбросы как отдельные элементы.

Преимущества:

* находит кластеры произвольной формы,
* хорошо работает с шумом.

---

### 4. **Модельная кластеризация**

Например, **Gaussian Mixture Models (GMM)**:

* предполагает, что данные — смесь распределений;
* использует EM-алгоритм.

Позволяет мягкую (probabilistic) кластеризацию.

---

## Оценка качества кластеризации

Поскольку нет истинных меток, используются внутренние метрики:

* **Silhouette Score** — насколько точка ближе к своему кластеру, чем к соседнему.
* **Davies-Bouldin Index** — чем меньше, тем лучше.
* **Calinski-Harabasz Index** — мера разделимости.

Для визуализации используют:

* PCA,
* t-SNE,
* UMAP.

---

## Проблемы кластеризации

* разный масштаб признаков → нужен StandardScaler;
* трудно выбрать правильное число кластеров;
* чувствительность к выбросам;
* отсутствие интерпретируемых меток.

---

## Итоги

Кластеризация — важный инструмент структурирования данных.
Алгоритмы различаются по:

* форме кластеров,
* устойчивости к шуму,
* вычислительной сложности.

Правильный выбор метода зависит от структуры данных и цели анализа.

---

# 12. OSINT


# Лекция по теме: OSINT: технологии, методики и подходы

## Введение

OSINT (Open Source Intelligence) — разведка на основе открытых источников.
Используется в:

* расследованиях,
* конкурентной разведке,
* кибербезопасности,
* анализе социальных сетей,
* выявлении угроз.

OSINT позволяет собирать данные открыто и легально, используя публичные источники.

---

## Основные источники OSINT

### 📌 1. Интернет-ресурсы

* новостные сайты,
* электронные СМИ,
* базы данных,
* электронные архивы.

### 📌 2. Социальные сети

* VK, Telegram, Facebook, Instagram, X (Twitter)
* анализ профилей, связей, активности.

### 📌 3. Государственные реестры

* юридические лица,
* ИП,
* имущество,
* судебные акты.

### 📌 4. Технические источники

* DNS-записи,
* WHOIS,
* утечки данных,
* анализ инфраструктуры.

---

## Методы OSINT

### 1. **Сбор данных**

* выгрузка сайтов,
* парсинг,
* анализ социальных графов,
* автоматический мониторинг.

### 2. **Фильтрация и очистка**

* удаление незначимых данных,
* классификация источников,
* оценка достоверности.

### 3. **Анализ и корреляция**

* сопоставление данных разных источников,
* выявление закономерностей,
* построение профилей.

### 4. **Визуализация**

* графы связей,
* временные диаграммы,
* карты.

---

## Популярные инструменты OSINT

* Maltego
* Shodan
* GHunt
* SpiderFoot
* Sherlock
* Recon-ng
* TheHarvester

---

## Ограничения OSINT

* достоверность данных не гарантирована;
* часто требуется ручная проверка;
* правовые ограничения;
* необходимость соблюдения этики и конфиденциальности.

---

## Итоги

OSINT — мощный метод анализа и разведки, позволяющий извлекать информацию из открытых источников.
Эффективность OSINT зависит от навыков аналитика, грамотной методики и использования автоматизированных инструментов.

---

# 13. СМО


# Лекция по теме: Системы массового обслуживания (СМО)

## Введение

Системы массового обслуживания (СМО) — математические модели, описывающие процессы:

* поступления задач (заявок),
* их ожидания,
* распределения по каналам обслуживания,
* обслуживания,
* выхода из системы.

СМО применяются для:

* моделирования сетей,
* проектирования ИТ-инфраструктуры,
* оценки нагрузки на сервисы,
* оптимизации производственных процессов.

---

## Основные элементы СМО

### ✔️ Входной поток

Определяет, как часто поступают заявки.
Часто моделируется распределением Пуассона.

### ✔️ Очередь

Заявки, ожидающие обслуживания.

### ✔️ Каналы обслуживания

Например:

* процессоры,
* кассы,
* сервисные станции.

### ✔️ Выходной поток

Обслуженные заявки.

---

## Параметры СМО

* **λ** — интенсивность входного потока;
* **μ** — интенсивность обслуживания;
* **ρ = λ / μ** — коэффициент загрузки;
* распределения времени: экспоненциальное, нормальное и др.

---

## Типы СМО

### 1. **M/M/1**

* пуассоновский поток,
* экспоненциальное обслуживание,
* один канал.

### 2. **M/M/n**

n параллельных каналов.

### 3. **M/M/∞**

Бесконечное число каналов (теоретическая модель).

### 4. **M/G/1**

Обслуживание произвольного распределения.

---

## Показатели эффективности

* среднее время ожидания в очереди;
* среднее число заявок в системе;
* вероятность отказа;
* вероятность простаивания канала;
* оптимальная длина очереди.

Эти показатели используются для планирования мощности систем.

---

## Применение СМО

* телекоммуникации (маршрутизация трафика),
* call-центры,
* ИТ-сервисы,
* медицинские учреждения,
* транспортные системы.

---

## Итоги

Системы массового обслуживания позволяют математически моделировать поведение больших потоков заявок и оптимизировать систему обслуживания.
Они применяются в ИТ, промышленности, логистике и сервисных организациях.

---
